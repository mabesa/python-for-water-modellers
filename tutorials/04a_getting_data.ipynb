{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Module 4a: Getting Hydrological Data\n",
    "## Finding and Accessing Open Streamflow Data\n",
    "\n",
    "**Time required:** 25-30 minutes  \n",
    "**Prerequisites:** Module 3a (Python basics), Module 3b (AI assistance)  \n",
    "**What you'll learn:** How to find and download hydrological data programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## The Data Challenge\n",
    "\n",
    "Ask any hydrologist about their biggest challenge, and you'll often hear: **\"Getting the data.\"**\n",
    "\n",
    "Before you can analyze discharge patterns, calibrate a model, or validate flood predictions, you need data. And getting that data is often harder than the analysis itself.\n",
    "\n",
    "### Why Is It So Difficult?\n",
    "\n",
    "| Challenge | Example |\n",
    "|-----------|--------|\n",
    "| **Different formats** | CSV, Excel, XML, proprietary formats, PDF tables |\n",
    "| **Different sources** | National agencies, research institutions, utilities |\n",
    "| **Access restrictions** | Some data requires registration, payment, or formal requests |\n",
    "| **Quality variations** | Gaps, errors, different QA procedures |\n",
    "| **Different conventions** | Date formats, time zones, units (mÂ³/s vs. L/s vs. cfs) |\n",
    "| **Documentation gaps** | Missing metadata about station location, catchment area |\n",
    "\n",
    "### The Good News\n",
    "\n",
    "The hydrological community has been working to make data more accessible. Several **large-scale, open datasets** now exist that provide:\n",
    "\n",
    "- Standardized formats\n",
    "- Consistent quality control\n",
    "- Rich metadata (catchment attributes)\n",
    "- Easy programmatic access\n",
    "\n",
    "In this module, we'll explore these resources and learn how to access them with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Open Hydrological Data Sources\n",
    "\n",
    "### Global and Continental Datasets\n",
    "\n",
    "**[Caravan](https://github.com/kratzert/Caravan)** ðŸŒ  \n",
    "A global collection of meteorological and hydrological data for large-sample hydrology. Combines data from multiple CAMELS datasets worldwide.\n",
    "- 6,830+ catchments globally\n",
    "- Daily streamflow + meteorological forcing\n",
    "- 65+ catchment attributes\n",
    "- Citation: Kratzert et al. (2023)\n",
    "\n",
    "**[GRDC (Global Runoff Data Centre)](https://grdc.bafg.de/)**  \n",
    "The world's largest archive of river discharge data.\n",
    "- 10,000+ stations worldwide\n",
    "- Long historical records (some > 100 years)\n",
    "- Requires registration for access\n",
    "\n",
    "**[GloFAS](https://global-flood.emergency.copernicus.eu/)** (Global Flood Awareness System)  \n",
    "Real-time and historical flood forecasting data from ECMWF/Copernicus.\n",
    "\n",
    "### The CAMELS Family\n",
    "\n",
    "CAMELS (**C**atchment **A**ttributes and **ME**teorology for **L**arge-sample **S**tudies) is a family of datasets designed for hydrological research. Each regional dataset follows similar standards:\n",
    "\n",
    "| Dataset | Region | Catchments | Reference |\n",
    "|---------|--------|------------|----------|\n",
    "| **CAMELS** | USA | 671 | Newman et al. (2015) |\n",
    "| **CAMELS-CL** | Chile | 516 | Alvarez-Garreton et al. (2018) |\n",
    "| **CAMELS-BR** | Brazil | 897 | Chagas et al. (2020) |\n",
    "| **CAMELS-GB** | Great Britain | 671 | Coxon et al. (2020) |\n",
    "| **CAMELS-AUS** | Australia | 222 | Fowler et al. (2021) |\n",
    "| **CAMELS-CH** | Switzerland | 331 | HÃ¶ge et al. (2023) |\n",
    "| **LamaH-CE** | Central Europe | 859 | Klingler et al. (2021) |\n",
    "\n",
    "> **Why CAMELS matters:**  \n",
    "> These datasets include not just streamflow, but also catchment characteristics (area, elevation, geology, land use) and meteorological forcing. This makes them ideal for testing models, machine learning, and comparative hydrology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Practical: Accessing CAMELS Data with Python\n",
    "\n",
    "We'll use **PyGeoHydro**, a Python package that provides easy access to various hydrological datasets, including CAMELS-US.\n",
    "\n",
    "### Step 1: Install PyGeoHydro\n",
    "\n",
    "In your project folder, add the package:\n",
    "\n",
    "```bash\n",
    "uv add pygeohydro\n",
    "```\n",
    "\n",
    "This may take a moment as it installs PyGeoHydro and its dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "### Step 2: Explore Available Stations\n",
    "\n",
    "Run the following code to see what's available in the CAMELS dataset. You can run it directly here in the notebook, or save it as `explore_camels.py` to run from the terminal with `uv run explore_camels.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:35:27.813734Z",
     "iopub.status.busy": "2026-02-04T21:35:27.813642Z",
     "iopub.status.idle": "2026-02-04T21:35:53.970854Z",
     "shell.execute_reply": "2026-02-04T21:35:53.970341Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Explore CAMELS dataset stations.\"\"\"\n",
    "\n",
    "import pygeohydro as gh\n",
    "\n",
    "print(\"Loading CAMELS dataset information...\")\n",
    "print()\n",
    "\n",
    "# Get CAMELS dataset (returns tuple: attributes DataFrame, streamflow Dataset)\n",
    "camels_attrs, camels_qobs = gh.get_camels()\n",
    "\n",
    "# Show available information\n",
    "print(f\"Number of stations: {len(camels_attrs)}\")\n",
    "print()\n",
    "\n",
    "# Show some columns available\n",
    "print(\"Available attributes (first 20):\")\n",
    "for i, col in enumerate(camels_attrs.columns[:20]):\n",
    "    print(f\"  {col}\")\n",
    "print(f\"  ... and {len(camels_attrs.columns) - 20} more\")\n",
    "print()\n",
    "\n",
    "# Show sample of station names\n",
    "print(\"Sample stations:\")\n",
    "print(camels_attrs[['gauge_name', 'q_mean']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "You should see:\n",
    "- 671 stations available\n",
    "- Many catchment attributes (area, precipitation, geology, etc.)\n",
    "- Station names and basic information\n",
    "\n",
    "> **First run:** The first time you run this, PyGeoHydro will download the CAMELS dataset (~140 MB). This is cached locally for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "### Step 3: Select a Station\n",
    "\n",
    "Let's find a station with interesting characteristics. The code below filters for medium-sized catchments with reasonable runoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:35:53.972137Z",
     "iopub.status.busy": "2026-02-04T21:35:53.971930Z",
     "iopub.status.idle": "2026-02-04T21:35:54.148818Z",
     "shell.execute_reply": "2026-02-04T21:35:54.148306Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Select a CAMELS station based on criteria.\"\"\"\n",
    "\n",
    "import pygeohydro as gh\n",
    "\n",
    "# Get CAMELS data\n",
    "camels_attrs, camels_qobs = gh.get_camels()\n",
    "\n",
    "print(\"Finding stations with specific characteristics...\")\n",
    "print()\n",
    "\n",
    "# Filter for medium-sized catchments with good data\n",
    "# area_gages2: catchment area in kmÂ²\n",
    "# q_mean: mean daily discharge in mm/day\n",
    "selected = camels_attrs[\n",
    "    (camels_attrs['area_gages2'] > 100) &      # At least 100 kmÂ²\n",
    "    (camels_attrs['area_gages2'] < 1000) &     # Not too large\n",
    "    (camels_attrs['q_mean'] > 1.0)             # Reasonable runoff\n",
    "].copy()\n",
    "\n",
    "print(f\"Stations matching criteria: {len(selected)}\")\n",
    "print()\n",
    "\n",
    "# Show top 10 by mean discharge\n",
    "top_stations = selected.nlargest(10, 'q_mean')\n",
    "print(\"Top 10 stations by mean runoff:\")\n",
    "print()\n",
    "\n",
    "for idx, row in top_stations.iterrows():\n",
    "    print(f\"  Station ID: {idx}\")\n",
    "    print(f\"  Name: {row['gauge_name']}\")\n",
    "    print(f\"  Area: {row['area_gages2']:.0f} kmÂ²\")\n",
    "    print(f\"  Mean runoff: {row['q_mean']:.2f} mm/day\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "Note down a station ID that looks interestingâ€”we'll download its data next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "### Step 4: Download Streamflow Data\n",
    "\n",
    "Now let's download actual streamflow data for a selected station. Change `STATION_ID` to the station you chose, or use the default example:\n",
    "\n",
    "> **Note:** PyGeoHydro returns the streamflow data as an **xarray Dataset**. xarray is a Python library for working with multi-dimensional data (like gridded climate data or time series across multiple stations). Think of it as a more powerful version of pandas that can handle dimensions beyond rows and columns. We'll convert it to a simpler pandas format for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:35:54.150076Z",
     "iopub.status.busy": "2026-02-04T21:35:54.149998Z",
     "iopub.status.idle": "2026-02-04T21:35:54.318893Z",
     "shell.execute_reply": "2026-02-04T21:35:54.318435Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Download streamflow data from CAMELS.\"\"\"\n",
    "\n",
    "import pygeohydro as gh\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration - change this to your chosen station\n",
    "# Note: We use station 01013500 (Fish River) as our example throughout these tutorials.\n",
    "# While it doesn't match the filter criteria from Step 3 (it's larger than 1000 kmÂ²),\n",
    "# it's a well-documented station with good data quality, making it ideal for learning.\n",
    "STATION_ID = \"01013500\"  # Example: Fish River near Fort Kent, Maine\n",
    "\n",
    "print(f\"Downloading data for station: {STATION_ID}\")\n",
    "print()\n",
    "\n",
    "# Get CAMELS data (attributes and streamflow)\n",
    "camels_attrs, camels_qobs = gh.get_camels()\n",
    "\n",
    "if STATION_ID not in camels_attrs.index:\n",
    "    print(f\"Error: Station {STATION_ID} not found in CAMELS dataset.\")\n",
    "    print(\"Check the station ID and try again.\")\n",
    "else:\n",
    "    station_info = camels_attrs.loc[STATION_ID]\n",
    "    area_km2 = station_info['area_gages2']\n",
    "    print(f\"Station: {station_info['gauge_name']}\")\n",
    "    print(f\"Area: {area_km2:.0f} kmÂ²\")\n",
    "    print(f\"Mean elevation: {station_info['elev_mean']:.0f} m\")\n",
    "    print()\n",
    "    \n",
    "    # Extract streamflow for this station from the xarray Dataset\n",
    "    print(\"Extracting streamflow data...\")\n",
    "    \n",
    "    # Get discharge data - raw CAMELS data is in cfs (cubic feet per second)\n",
    "    Q_cfs = camels_qobs['discharge'].sel(station_id=STATION_ID)\n",
    "    \n",
    "    # Convert to pandas Series\n",
    "    Q_cfs = Q_cfs.to_series()\n",
    "    \n",
    "    # Convert cfs to mÂ³/s (1 cfs = 0.0283168 mÂ³/s)\n",
    "    Q_m3s = Q_cfs * 0.0283168\n",
    "    \n",
    "    # Also calculate mm/day for reference\n",
    "    Q_mm_day = Q_m3s * 86400 / (area_km2 * 1e6) * 1000\n",
    "    \n",
    "    print(f\"Retrieved {len(Q_m3s)} daily values\")\n",
    "    print(f\"Period: {Q_m3s.index.min().strftime('%Y-%m-%d')} to {Q_m3s.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Missing values: {Q_m3s.isna().sum()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Basic statistics (mÂ³/s):\")\n",
    "    print(f\"  Mean:   {Q_m3s.mean():.2f}\")\n",
    "    print(f\"  Min:    {Q_m3s.min():.2f}\")\n",
    "    print(f\"  Max:    {Q_m3s.max():.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21cf37",
   "metadata": {},
   "source": [
    "### ðŸ“ Understanding File Paths\n",
    "\n",
    "Before we save our data, let's understand how Python finds files on your computer.\n",
    "\n",
    "#### What is a File Path?\n",
    "\n",
    "A **path** tells Python where to find (or save) a file. There are two types:\n",
    "\n",
    "**Absolute Path** (Full address from the root of your computer)\n",
    "```python\n",
    "'/Users/yourname/Documents/project/data/my_data.csv'  # macOS/Linux\n",
    "'C:\\\\Users\\\\yourname\\\\Documents\\\\project\\\\data\\\\my_data.csv'  # Windows\n",
    "```\n",
    "\n",
    "**Relative Path** (Address relative to your current location)\n",
    "```python\n",
    "'../data/my_data.csv'  # Go up one folder, then into data/\n",
    "```\n",
    "\n",
    "#### Our Project Structure\n",
    "\n",
    "```\n",
    "python-for-water-modellers/          â† Project root\n",
    "â”œâ”€â”€ data/                            â† Data folder\n",
    "â”‚   â””â”€â”€ camels_01013500_discharge.csv                  \n",
    "â”œâ”€â”€ tutorials/                       â† We are HERE (this notebook)\n",
    "â”‚   â”œâ”€â”€ 04a_getting_data.ipynb      \n",
    "â”‚   â””â”€â”€ 04b_discharge_analysis.ipynb\n",
    "â””â”€â”€ src/python_for_water_modellers/  â† Reusable utilities\n",
    "    â””â”€â”€ paths.py                     â† We save our helper here!\n",
    "```\n",
    "\n",
    "When running **locally** in VS Code, the notebook runs from `tutorials/`, so we use `../data/` to go up one level.\n",
    "\n",
    "When running in **Binder** (cloud), the working directory is the project root, so we use `data/` directly.\n",
    "\n",
    "#### Smart Path Resolution\n",
    "\n",
    "The code below automatically detects which environment you're in and uses the correct path.\n",
    "\n",
    "> **Reusable Code:** We've saved this function in `src/python_for_water_modellers/paths.py` so you can import it in other notebooks with `from python_for_water_modellers import get_data_path`. Here we show the full implementation for learning purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef15a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:35:54.320214Z",
     "iopub.status.busy": "2026-02-04T21:35:54.320125Z",
     "iopub.status.idle": "2026-02-04T21:35:54.322529Z",
     "shell.execute_reply": "2026-02-04T21:35:54.322108Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_data_path():\n",
    "    \"\"\"\n",
    "    Get the correct path to the data folder.\n",
    "    Works both locally (VS Code) and in Binder.\n",
    "    \"\"\"\n",
    "    # Check if running in Binder (cloud environment)\n",
    "    if 'BINDER_REQUEST' in os.environ or 'BINDER_LAUNCH_HOST' in os.environ:\n",
    "        # Binder: data is at ~/data/ (repo is cloned to home directory)\n",
    "        data_path = Path.home() / 'data'\n",
    "    elif Path('../data').exists():\n",
    "        # Local: notebook is in tutorials/, data is at ../data/\n",
    "        data_path = Path('../data')\n",
    "    elif Path('data').exists():\n",
    "        # Alternative: already at repo root\n",
    "        data_path = Path('data')\n",
    "    else:\n",
    "        # Fallback\n",
    "        data_path = Path('../data')\n",
    "    \n",
    "    return data_path\n",
    "\n",
    "# Get the data path for this environment\n",
    "DATA_PATH = get_data_path()\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Data path exists: {DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clgpzz2vb79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:35:54.323730Z",
     "iopub.status.busy": "2026-02-04T21:35:54.323644Z",
     "iopub.status.idle": "2026-02-04T21:35:54.354999Z",
     "shell.execute_reply": "2026-02-04T21:35:54.354617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the data using the detected path\n",
    "output_df = pd.DataFrame({\n",
    "    'date': Q_m3s.index,\n",
    "    'discharge_m3s': Q_m3s.values,\n",
    "    'discharge_mm_day': Q_mm_day.values\n",
    "})\n",
    "output_df['station_id'] = STATION_ID\n",
    "output_df['station_name'] = station_info['gauge_name']\n",
    "\n",
    "# Use the DATA_PATH we determined above\n",
    "output_file = DATA_PATH / f'camels_{STATION_ID}_discharge.csv'\n",
    "output_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data saved to: {output_file}\")\n",
    "print()\n",
    "print(\"You can now use this file in Module 4b for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ef354",
   "metadata": {},
   "source": [
    "### Step 5: Quick Visualization\n",
    "\n",
    "Let's make a quick plot to verify the data looks reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:35:54.356221Z",
     "iopub.status.busy": "2026-02-04T21:35:54.356144Z",
     "iopub.status.idle": "2026-02-04T21:35:55.011596Z",
     "shell.execute_reply": "2026-02-04T21:35:55.011229Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Quick visualization of downloaded streamflow data.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data we just downloaded using DATA_PATH\n",
    "INPUT_FILE = DATA_PATH / f'camels_{STATION_ID}_discharge.csv'\n",
    "\n",
    "print(f\"Loading: {INPUT_FILE}\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "\n",
    "station_name = df['station_name'].iloc[0]\n",
    "\n",
    "# Create a simple plot\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot 3 years of data for clarity\n",
    "df_subset = df['2010':'2012']\n",
    "\n",
    "ax.plot(df_subset.index, df_subset['discharge_m3s'], 'b-', linewidth=0.5)\n",
    "ax.fill_between(df_subset.index, 0, df_subset['discharge_m3s'], alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Discharge (mÂ³/s)')\n",
    "ax.set_title(f'{station_name}\\nDaily Discharge 2010-2012')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot displayed above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609bfab",
   "metadata": {},
   "source": [
    "**What you should see:**\n",
    "- A hydrograph showing the seasonal pattern of streamflow\n",
    "- Higher flows in spring (snowmelt) for mountain catchments\n",
    "- Lower flows in late summer/autumn\n",
    "- Individual flood peaks from storm events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a8b9",
   "metadata": {},
   "source": [
    "## For Swiss-Specific Work: CAMELS-CH\n",
    "\n",
    "If you're working specifically with Swiss catchments, you'll want to use **CAMELS-CH**.\n",
    "\n",
    "### What's in CAMELS-CH?\n",
    "\n",
    "- **331 catchments** across Switzerland\n",
    "- Daily streamflow data (1981-2020)\n",
    "- 211 catchment attributes (topography, climate, geology, land cover, hydrology)\n",
    "- Comprehensive metadata\n",
    "\n",
    "### How to Access\n",
    "\n",
    "CAMELS-CH is available from Zenodo:\n",
    "\n",
    "1. Visit: https://doi.org/10.5281/zenodo.7784633\n",
    "2. Download the dataset files\n",
    "3. The data comes as CSV files that you can load with pandas\n",
    "\n",
    "### Example Loading Code\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# After downloading and extracting CAMELS-CH\n",
    "# Load catchment attributes\n",
    "attributes = pd.read_csv('CAMELS_CH_catchment_attributes.csv')\n",
    "\n",
    "# Load streamflow for a specific station\n",
    "streamflow = pd.read_csv('CAMELS_CH_streamflow_daily.csv')\n",
    "\n",
    "# Filter for your station of interest\n",
    "station_id = '2004'  # Example: Aare at Ringgenberg\n",
    "my_data = streamflow[streamflow['station_id'] == station_id]\n",
    "```\n",
    "\n",
    "### Citation\n",
    "\n",
    "When using CAMELS-CH in publications, please cite:\n",
    "\n",
    "> HÃ¶ge, M., Kauzlaric, M., Siber, R., SchÃ¶nenberger, U., Horton, P., Schwanbeck, J., Floriancic, M. G., Viviroli, D., Wilhelm, S., Sikorska-Senoner, A. E., Addor, N., Brunner, M., Pool, S., Zappa, M., & Fenicia, F. (2023). CAMELS-CH: Hydro-meteorological time series and landscape attributes for 331 catchments in hydrologic Switzerland. *Earth System Science Data*, 15, 5755â€“5784."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8b9c0",
   "metadata": {},
   "source": [
    "## Data Quality Considerations\n",
    "\n",
    "Whenever you work with hydrological dataâ€”whether from CAMELS, national databases, or local sourcesâ€”always check:\n",
    "\n",
    "### 1. Missing Values\n",
    "\n",
    "```python\n",
    "# Count missing values\n",
    "missing = df['discharge'].isna().sum()\n",
    "print(f\"Missing: {missing} days ({missing/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Where are they?\n",
    "missing_periods = df[df['discharge'].isna()]\n",
    "```\n",
    "\n",
    "### 2. Suspicious Values\n",
    "\n",
    "```python\n",
    "# Check for zeros (often indicate sensor issues)\n",
    "zeros = (df['discharge'] == 0).sum()\n",
    "\n",
    "# Check for negative values (physically impossible)\n",
    "negatives = (df['discharge'] < 0).sum()\n",
    "\n",
    "# Check for extreme spikes (potential errors)\n",
    "mean_q = df['discharge'].mean()\n",
    "std_q = df['discharge'].std()\n",
    "extremes = df[df['discharge'] > mean_q + 5*std_q]\n",
    "```\n",
    "\n",
    "### 3. Time Series Continuity\n",
    "\n",
    "```python\n",
    "# Check for gaps in the time series\n",
    "date_diff = df.index.to_series().diff()\n",
    "gaps = date_diff[date_diff > pd.Timedelta(days=1)]\n",
    "print(f\"Found {len(gaps)} gaps in the time series\")\n",
    "```\n",
    "\n",
    "### 4. Physical Plausibility\n",
    "\n",
    "- Is the catchment area correct?\n",
    "- Do peak flows coincide with precipitation events?\n",
    "- Is the seasonal pattern reasonable for the climate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c0d1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "âœ… Why getting hydrological data is challenging  \n",
    "âœ… Key open data sources (CAMELS family, GRDC, Caravan)  \n",
    "âœ… How to access CAMELS data programmatically with PyGeoHydro  \n",
    "âœ… How to explore, select, and download station data  \n",
    "âœ… How to save data for offline analysis  \n",
    "âœ… Where to find CAMELS-CH for Swiss-specific work  \n",
    "âœ… What to check for data quality  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Open datasets exist** â€” You don't always need to process raw data from scratch\n",
    "2. **Python makes access easy** â€” A few lines of code can replace hours of manual downloads\n",
    "3. **Always check quality** â€” Even curated datasets may have issues\n",
    "4. **Save locally** â€” Once you have good data, save it for reproducibility\n",
    "\n",
    "### What You Created\n",
    "\n",
    "By running the code cells in this notebook, you:\n",
    "- Explored available CAMELS stations\n",
    "- Downloaded streamflow data for your chosen station\n",
    "- Created a CSV file: `camels_XXXXX_discharge.csv` in the `data/` folder\n",
    "- Generated a preview plot of the hydrograph\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Use your downloaded data for a complete analysis â†’ [Module 4b: Your First Water Modelling Script](04b_discharge_analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
